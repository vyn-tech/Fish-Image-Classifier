{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b962571-9563-4c2c-acde-22479c9c5fd1",
   "metadata": {},
   "source": [
    "#### Build a Classifier with Pytorch\n",
    "Now that our data has been cleaned and preprocess, we can build a model. \n",
    "First, we will split the data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da873d2-f1e3-43c1-be99-3b0770cd5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packagaes/libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0b0367-b1bb-497d-a338-a5dd34a69fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "project_folder = r'C:\\Users\\Vy\\Documents\\Lewisville_Lake_Fish _Classification'\n",
    "\n",
    "# fish folders \n",
    "base = os.path.join(project_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7286edfc-5551-4e07-8069-4845e860dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(base_dir, output_dir=base, test_size=0.2): # file structure Data -> train, test, \"the fish species\"\n",
    "    for species in os.listdir(base_dir):\n",
    "        files = os.listdir(os.path.join(base_dir, species))\n",
    "        train_files, test_files = train_test_split(files, test_size=test_size, random_state=42)\n",
    "        \n",
    "        for split, split_files in zip(['train', 'test'], [train_files, test_files]): \n",
    "            split_dir = os.path.join(output_dir, split, species) # creates directory for current split and species\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "            for file in split_files:\n",
    "                src = os.path.join(base_dir, species, file) # source\n",
    "                dst = os.path.join(split_dir, file) # destination\n",
    "                shutil.copy(src, dst) # copies files from source to destination\n",
    "                \n",
    "# split\n",
    "split_dataset(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760a5d44-79cf-406d-9df6-1831ab4ad26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906cf7f7-23a5-494b-a354-8ecc302d4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = datasets.ImageFolder(r'C:\\Users\\Vy\\Documents\\Lewisville_Lake_Fish _Classification\\Data\\train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(r'C:\\Users\\Vy\\Documents\\Lewisville_Lake_Fish _Classification\\Data\\test', transform=transform)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ab0a53-d4f0-439a-91f9-ee0a75ff5e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128])\n",
      "tensor([1, 3, 1, 3, 1])\n",
      "['Guadalupe_bass', 'Largemouth_bass', 'Smallmouth_bass', 'Spotted_bass']\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)   # e.g. torch.Size([32, 3, 128, 128]), batch size, rgb channels, image size, respectively\n",
    "print(labels[:5])     # e.g. tensor([0, 2, 1, 0, 3]), labels for classes\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f5722e-cf8a-4533-9e4e-e8afbecd05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FishClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FishClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> 64x64\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> 32x32\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55236c08-b29f-4bfc-ae9f-dcc2595aab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 17.5068\n",
      "Epoch 2, Loss: 8.6008\n",
      "Epoch 3, Loss: 7.8628\n",
      "Epoch 4, Loss: 7.6472\n",
      "Epoch 5, Loss: 7.1556\n",
      "Epoch 6, Loss: 6.3986\n",
      "Epoch 7, Loss: 4.9108\n",
      "Epoch 8, Loss: 4.1091\n",
      "Epoch 9, Loss: 3.0173\n",
      "Epoch 10, Loss: 2.2380\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FishClassifier(num_classes=len(train_dataset.classes)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4834ec7-33ec-4eaa-a8c3-cfeaa9545423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1765a6fa-308b-4c47-bc66-1f5d9b0e2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Guadalupe_bass       0.33      0.67      0.44         9\n",
      "Largemouth_bass       0.50      0.50      0.50        12\n",
      "Smallmouth_bass       0.44      0.31      0.36        13\n",
      "   Spotted_bass       0.33      0.18      0.24        11\n",
      "\n",
      "       accuracy                           0.40        45\n",
      "      macro avg       0.40      0.41      0.39        45\n",
      "   weighted avg       0.41      0.40      0.38        45\n",
      "\n",
      "[[6 2 1 0]\n",
      " [2 6 2 2]\n",
      " [4 3 4 2]\n",
      " [6 1 2 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()  # set model to eval mode\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# generate classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee33bf-746e-4573-9f08-245926ba3957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
